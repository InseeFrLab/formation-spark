{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction à Spark\n",
    "\n",
    "Ce module introductif vise à effleurer l'intêret de l'utilisation de Spark et son utilisation au sein du datalab.\n",
    "En un mot je dirai que spark permet d'écrire du code de la même façon  pour son execution sur une machine unique d'un datascientist ou sur un cluster avec des milliers de cpu. Alors si ca vous tente de voir comment ca tourne.. let's go\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contexte spark\n",
    "Les différentes méthodes d'analyse de données s'appuie sur un **contexte Spark** qui masque la complexité de l'infrastructure sous-jacente au Datascientist. \n",
    "\n",
    "Ce contexte spark donne accès à un **driver spark** ( qui tourne dans une JVM scala ) qui lui même a accès à des executor auquels seront distribués les jobs. Ces executor sont obtenues à partir d'un **ressource manager**.\n",
    "Il y a 5 possibilités :\n",
    "* local : le cas où le driver crée sur la machine des threads en utilisant la cpu de la machine local\n",
    "* spark standalone : un ressource manager basique fournit par spark.\n",
    "* mesos : le ressource manager historique provenant de l'université de Berkeley \n",
    "* yarn : le ressource manager présent dans les infras Big data traditionnelles (cluster cloudera/hortonworks par exemple)\n",
    "* kubernetes : le petit nouveau, l'ogre de l'orchestration de container."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Langage de développement\n",
    "Il existe plusieurs langage pour interargir avec un driver spark :\n",
    "- scala (natif : la voie royale) \n",
    "- python au travers du module pyspark\n",
    "- r avec sparkr par exemple\n",
    "- java\n",
    "\n",
    "Il existe même des outils permettant de dialoguer uniquement en sql vers spark par l'intermédiation du Thrift JDBC/ODBC server.\n",
    "Bref nous nous focaliserons sur **python** ici."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Où êtes vous ?\n",
    "Vous etes sur une image docker construite ici https://github.com/InseeFrLab/jupyter-datascience\n",
    "\n",
    "Vous avez une installation de spark  et de hadoop deux projets incontournables de l'ecosystème big data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hadoop 3.3.1\n",
      "Source code repository https://github.com/apache/hadoop.git -r a3b9c37a397ad4188041dd80621bdeefc46885f2\n",
      "Compiled by ubuntu on 2021-06-15T05:13Z\n",
      "Compiled with protoc 3.7.1\n",
      "From source with checksum 88a4ddb2299aca054416d6b7f81ca55\n",
      "This command was run using /opt/hadoop/share/hadoop/common/hadoop-common-3.3.1.jar\n"
     ]
    }
   ],
   "source": [
    "!hadoop version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark 3.2.0 (git revision 5d45a41) built for Hadoop 3.3.1\n",
      "Build flags: -Phadoop-provided -Phive -Phive-thriftserver -Pkubernetes -Dhadoop.version=3.3.1 -Dhive.version=2.3.9\n"
     ]
    }
   ],
   "source": [
    "cat /opt/spark/RELEASE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qu'allons nous faire ?\n",
    "Dans ce premier module d'une longue série, nous allons simplement :\n",
    "* créer un **contexte spark local**\n",
    "* executer les exemples présents sur le site https://spark.apache.org/examples.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession \n",
    "         .builder\n",
    "         .appName(\"Datalake TP\")\n",
    "         .master(\"local[5]\")\n",
    "         .getOrCreate()\n",
    "        )\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un driver spark expose une IHM web de suivi des jobs, des executors.. Le lien html ci dessus vous permet d'y accèder.\n",
    "Nous aurons tout le temps d'y revenir, commençons à executer l'exemple pour l'approximation de Pi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calcul de Pi\n",
    "\n",
    "Très marrant ce calcul de PI qui consiste à estimer PI en probabilité en tirant des points du carré [0,1]x[0,1] et en regardant ceux qui atterissent dans le quart de cercle de rayon 1 centré sur le point (0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.121200\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "NUM_SAMPLES = 10000\n",
    "\n",
    "def inside(p):\n",
    "    x, y = random.random(), random.random()\n",
    "    return x*x + y*y < 1\n",
    "\n",
    "count = sc.parallelize(range(0, NUM_SAMPLES)) \\\n",
    "             .filter(inside).count()\n",
    "print (\"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La convergence en probabilité est très faible, ce qui illustre si besoin que ce n'est parce qu'on distribue des calculs qu'on fait des choses intelligentes avec :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**technique :** dans cet exemple nous avons distribué les entiers de 0 à NUM_SAMPLES avec la fonction parrallelize. En pratique chaque executeur à reçu son lot d'entiers de la part du driver. Le driver a en mémoire la liste complète.. cette parallèlisation ne passe pas vraiment à l'échelle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wordcount\n",
    "\n",
    "Exemple ultra classique coté Big Data, le comptage de mot.\n",
    "On va compter les mots de ce module pour voir de quoi il parle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour cela nous allons télécharger un fichier de données avec l'outil en ligne de commande mc (existant dans chaque service du datalab).\n",
    "Evidemment il aurait été possible d'utiliser des apis pythons pour cela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..._2021.csv:  2.68 MiB / 2.68 MiB ┃▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓┃ 8.34 MiB/s 0s\u001b[0m\u001b[0m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m"
     ]
    }
   ],
   "source": [
    "!mc cp s3/projet-spark-lab/diffusion/formation/data/trump-tweets/trump_insult_tweets_2014_to_2021.csv ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 16501),\n",
       " ('and', 9009),\n",
       " ('to', 8773),\n",
       " ('of', 7064),\n",
       " ('a', 6995),\n",
       " ('is', 6904),\n",
       " ('in', 4944),\n",
       " ('for', 3896),\n",
       " ('that', 3405),\n",
       " ('are', 3319),\n",
       " ('on', 3151),\n",
       " ('&', 2914),\n",
       " ('I', 2741),\n",
       " ('be', 2522),\n",
       " ('have', 2464),\n",
       " ('with', 2323),\n",
       " ('was', 2245),\n",
       " ('they', 2114),\n",
       " ('Fake', 2009),\n",
       " ('will', 1941)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_file = sc.textFile(\"trump_insult_tweets_2014_to_2021.csv\")\n",
    "counts = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(lambda a, b: a + b) \\\n",
    "             .sortBy(lambda a : - a[1] )\n",
    "\n",
    "counts.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce qui nous montre plein de choses :\n",
    "* qu'il y a pas mal de travail pour analyser un texte et le nettoyer\n",
    "* que Fake semble être le mot signifiant le plus représenté"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**technique :** Ici la source est un fichier local, il faut que chaque executor ait accès à ce fichier localement ce qui est bien plus compliqué à réaliser lorsqu'on travaille en cluster via un ressource manager."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pensez à éteindre votre SparkSession lorsque vous avez terminé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
