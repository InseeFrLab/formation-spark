---
title: "SparkR"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Objectifs

Cette formation va reprendre les exemples des tutoriaux pyspark sur parquet et hivemetastore.
Il est fortement recommandé de parcourir les formations précedentes en python (pyspark). Les concepts liés à Spark sont indépendant du langage pour "appeler" Spark. 

L'objectif est donc de voir comment avoir recours à Spark depuis R grace à l'API SparkR. 
Une librairie d'accès concurrente existe "Sparklyr"

Je prefère présenter la lirairie d'accès proposer par le projet Apache Spark.

## Initialisation du contexte Spark

Lors d'une phase de developpement sur donnée volumineuse, il est interessant d'utiliser une configuration dynamique du nombre d'executeurs afin de relacher les ressources lorsque l'on execute pas de taches.

```{r spark, echo=FALSE}
library(SparkR)

# Initialize SparkSession
sparkR.session(master = "k8s://https://kubernetes.default.svc:443",
               appName = "SparkR",
               sparkHome = Sys.getenv("SPARK_HOME"),
               list(spark.kubernetes.container.image=Sys.getenv("IMAGE_NAME"), 
                    spark.kubernetes.authenticate.driver.serviceAccountName=Sys.getenv("KUBERNETES_SERVICE_ACCOUNT"),
                    spark.kubernetes.driver.pod.name=Sys.getenv("KUBERNETES_POD_NAME"),
                    spark.kubernetes.namespace="user-tm8enk",
                    # config pour allocation dynamique
                    spark.dynamicAllocation.enabled="true",
                    spark.dynamicAllocation.initialExecutors="1",
                    spark.dynamicAllocation.minExecutors="1",
                    spark.dynamicAllocation.maxExecutors="10",
                    spark.dynamicAllocation.executorAllocationRatio="1",
                    spark.dynamicAllocation.shuffleTracking.enabled="true"
                    )
)
```

On peut vérifier la présence de notre executeur spark R


```{bash , echo=TRUE}
kubectl get pods -l spark-role=executor
```
## accès à un fichier csv

Attention la lecture d'un fichier avec inférence du schéma est couteuse car l'ensemble du fichier va etre lu pour en déduire le schéma.

```{r csv, echo=FALSE}
df <- read.df("s3a://projet-spark-lab/diffusion/formation/data/sirene/sirene.csv", "csv", header = "true", inferSchema = "true")
```

```{r csv, echo=TRUE}
head(df)
```
## manipulation du dataframe avec l'api SparkR

```{r, echo=FALSE}
ape_count <- summarize(groupBy(df, df$activitePrincipaleEtablissement), count = n(df$activitePrincipaleEtablissement))
head(arrange(ape_count, desc(ape_count$count)))
```
## accès à un fichier

Ici on va enregistrer la table en lui donnant un nom (sirene) puis l'utiliser dans une requête sql
```{r, echo=TRUE}
createOrReplaceTempView(df, "sirene")
ape_count_sql<- sql("SELECT count(*) as tot , activitePrincipaleEtablissement FROM sirene group by activitePrincipaleEtablissement order by tot desc;")
head(ape_count_sql)
```

## accès à un fichier parquet

Le format parquet est idéal pour manipuler des données volumineuses.
Le schéma est présent dans le format, pas besoin d'inférer.
```{r, echo=FALSE}
dfparquet <- read.df("s3a://projet-spark-lab/diffusion/formation/data/sirene.parquet", "parquet")
createOrReplaceTempView(dfparquet, "sireneparquet")
```

```{r, echo=TRUE}
ape_count_parquet<- sql("SELECT count(*) as tot , activitePrincipaleEtablissement FROM sireneparquet group by activitePrincipaleEtablissement order by tot desc;")
head(ape_count_parquet)
```
Comme en python, seules les colonnes nécessaires à l'exploitation de la requête (activitePrincipaleEtablissement) n'a été téléchargée depuis le stockage ce qui est beaucoup plus rapide

```{r, echo=FALSE}

sparkR.session.stop()

```

