{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark graph\n",
    "\n",
    "Dans les tutoriels précédents nous avons toujours travaillé avec des dataset ou table.\n",
    "\n",
    "Il existe un écosystème spark pour traiter de gros graphes via l'api **graphx** historiquement que disponible en scala il existe **graphFrames** en python.\n",
    "\n",
    "Essayons de jouer avec cette api.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création du context Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7fa270555850>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import os\n",
    "\n",
    "conf = SparkConf()\n",
    "\n",
    "#url par défaut d'une api kubernetes accédé depuis l'intérieur du cluster (ici le notebook tourne lui même dans kubernetes)\n",
    "conf.setMaster(\"k8s://https://kubernetes.default.svc:443\")\n",
    "\n",
    "#image des executors spark: pour des raisons de simplicité on réutilise l'image du notebook\n",
    "conf.set(\"spark.kubernetes.container.image\", \"inseefrlab/jupyter-datascience:master\")\n",
    "\n",
    "# Nom du compte de service pour contacter l'api kubernetes : attention le package du datalab crée lui même cette variable d'enviromment.\n",
    "# Dans un pod du cluster kubernetes il faut lire le fichier /var/run/secrets/kubernetes.io/serviceaccount/token\n",
    "# Néanmoins ce paramètre est inutile car le contexte kubernetes local de ce notebook est préconfiguré\n",
    "# conf.set(\"spark.kubernetes.authenticate.driver.serviceAccountName\", os.environ['KUBERNETES_SERVICE_ACCOUNT']) \n",
    "\n",
    "# Nom du namespace kubernetes\n",
    "conf.set(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE'])\n",
    "\n",
    "# Nombre d'executeur spark, il se lancera autant de pods kubernetes que le nombre indiqué.\n",
    "conf.set(\"spark.executor.instances\", \"10\")\n",
    "\n",
    "# Mémoire alloué à la JVM\n",
    "# Attention par défaut le pod kubernetes aura une limite supérieur qui dépend d'autres paramètres.\n",
    "# On manipulera plus bas pour vérifier la limite de mémoire totale d'un executeur\n",
    "conf.set(\"spark.executor.memory\", \"4g\")\n",
    "\n",
    "conf.set(\"spark.kubernetes.driver.pod.name\", os.environ['KUBERNETES_POD_NAME'])\n",
    "\n",
    "# Paramètres d'enregistrement des logs spark d'application\n",
    "# Attention ce paramètres nécessitent la création d'un dossier spark-history. Spark ne le fait pas lui même pour des raisons obscurs\n",
    "# import s3fs\n",
    "# endpoint = \"https://\"+os.environ['AWS_S3_ENDPOINT']\n",
    "# fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': endpoint})\n",
    "# fs.touch('s3://tm8enk/spark-history/.keep')\n",
    "# sparkconf.set(\"spark.eventLog.enabled\",\"true\")\n",
    "# sparkconf.set(\"spark.eventLog.dir\",\"s3a://tm8enk/spark-history\")\n",
    "#ici pour gérer le dateTimeFormatter dépendant de la verion de java...\n",
    "conf.set(\"spark.sql.legacy.timeParserPolicy\",\"LEGACY\")\n",
    "conf.set(\"spark.default.parallelism\",10)\n",
    "conf.set(\"spark.sql.shuffle.partitions\",10)\n",
    "conf.set(\"spark.jars.packages\",\"graphframes:graphframes:0.8.1-spark3.0-s_2.12\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On note que :\n",
    "* on a pris 10 executeurs et on surcharge spark pour, lors de shuffle ou repartition, que son niveau de parallelisme soit 10 plutot que 200\n",
    "* on importe un jar au démarrage il va aller le chercher sur maven central, ce jar sera appellé par la librairie python graphFrames à travers Py4j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"graph\").config(conf = conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting graphframes\n",
      "  Downloading graphframes-0.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting nose\n",
      "  Downloading nose-1.3.7-py3-none-any.whl (154 kB)\n",
      "\u001b[K     |████████████████████████████████| 154 kB 5.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from graphframes) (1.18.1)\n",
      "Installing collected packages: nose, graphframes\n",
      "Successfully installed graphframes-0.6 nose-1.3.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install graphframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constitution d'un graphe\n",
    "\n",
    "Un graphe peut être vu comme 2 datasets:\n",
    "* Un **dataset de noeud** avec 2 colonnes : un identifiant et un attribut.\n",
    "* Un **dataset d'arcs** reliant 2 noeuds : avec un identifiant de noeud source, un destination et un ou des attributs sur cet arc\n",
    " \n",
    "A partir de ces 2 datasets que spark peut traiter comme des rdd à travers le cluster, on peut travailler sur un graphe, voici un exemple simple :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphframes import *\n",
    "\n",
    "vertices = [(1,\"A\"), (2,\"B\"), (3, \"C\")]\n",
    "edges = [(1,2,\"love\"), (2,1,\"hate\"), (2,3,\"follow\")]\n",
    "\n",
    "v = spark.createDataFrame(vertices, [\"id\", \"name\"])\n",
    "e = spark.createDataFrame(edges, [\"src\", \"dst\", \"action\"])\n",
    "\n",
    "premierGraphe = GraphFrame(v, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+\n",
      "|src|dst|action|\n",
      "+---+---+------+\n",
      "|  1|  2|  love|\n",
      "|  2|  1|  hate|\n",
      "|  2|  3|follow|\n",
      "+---+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "premierGraphe.edges.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faisons un graphe sur les données twitter concernant l'insee\n",
    "\n",
    "Attention les données sont alimentées et à la réexecution du notebook les éléments suivants auront peut-être évolués, on peut filtrer sur la date des tweets si nécessaire avant le 16 avril pour retrouver le déroulé ci-dessous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on importe le schema\n",
    "import pickle\n",
    "schema = pickle.load( open( \"./7-streaming/schema.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on lit les donnnées\n",
    "df = spark.read.format(\"json\")  \\\n",
    "    .schema(schema) \\\n",
    "    .load(\"s3a://projet-spark-lab/diffusion/tweets/input\") \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essayons de créer le graph des tweets mentionnant l'insee avec comme noeud les user de twitter et comme arc, 2 utilisateurs sont reliés si l'émetteur du tweet a metionné un autre utilisateur dans le tweet\n",
    "\n",
    "Par exemple si alice tweet \"@bob tu vas vu le dernier tweet de l'Insee\" le noeud alice sera relié -> à bob.\n",
    "\n",
    "Nous avons de la chance les données de l'api twitter prémache le travail.\n",
    "\n",
    "Regardons pour commencer dans le tweet de l'api twitter les données suivantes:\n",
    "* l'identififiant du tweet\n",
    "* l'identifiant de l'utilisateur qui tweete\n",
    "* son nombre de followers pour savoir s'il est influent\n",
    "* la liste des hashtags présents dans le tweet (déjà donné par twitter)\n",
    "* une indicatrice pour savoir s'il y a des hashtag \n",
    "* le texte du tweet\n",
    "* les user mentionnés dans le tweet sous forme de tableau (identifiant, name, screen_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1380418192426860546, user_id=219397176, name='Isabelle Hinden', followers_count=1633, hashtags=['immigrés'], has_hashtag=1, text='RT @InseeFr: En 2017, 44 % de la hausse de la population provient des #immigrés, soit 139\\xa0000 personnes. Depuis 2006, cette contribution à…', user_mentions=[Row(id=217473382, id_str='217473382', indices=[3, 11], name='Insee', screen_name='InseeFr')])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,explode,size,first\n",
    "#.select(\"id\",\"user.id\",\"user.name\",\"user.followers_count\",\"entities.hashtags\",\"text\")\n",
    "df.select(col(\"id\").alias(\"id\"),\n",
    "         col(\"user.id\").alias(\"user_id\"),\n",
    "         col(\"user.name\").alias(\"name\"),\n",
    "         col(\"user.followers_count\"),\n",
    "         col(\"entities.hashtags.text\").alias(\"hashtags\"),\n",
    "         size(\"entities.hashtags\").alias(\"has_hashtag\"),\n",
    "         col(\"text\"),\n",
    "         col(\"entities.user_mentions\")) \\\n",
    " .filter(col(\"has_hashtag\")>0) \\\n",
    " .head(1) \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour créer le dataset des noeuds:\n",
    "* on récupére les id et noms des émetteurs de tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vertices\n",
    "tweeters=df.select(col(\"user.id\").alias(\"id\"), col(\"user.name\").alias(\"name\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* On récupère les id et noms des user mentionnés (même s'ils n'ont pas forcément twittés)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_mentionned=df.select(explode(col(\"entities.user_mentions\"))).select(col(\"col.id\").alias(\"id\"),col(\"col.screen_name\").alias(\"name\")).distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On fait l'union des 2 dataframe et on va ne retenir qu'un nom (le nom dans la mention n'est pas toujours exact au nom du compte apparemment ca évitera les doublons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices=tweeters.union(users_mentionned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, collect_list\n",
    "# on définit un udf qui prend le premier élément\n",
    "udf_first = udf(lambda x: x[0])\n",
    "# on groupe l'union des deux dataset par id d'utilisateur et on collect \n",
    "#sous la forme d'une liste leurs noms qui peuvent différer entre les mentions et le compte\n",
    "# on applique l'udf pour ne retenir que le nom en tête\n",
    "\n",
    "final_vertices=vertices.groupby(\"id\").agg(udf_first(collect_list(\"name\")).alias(\"name\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------------------------+\n",
      "|id      |name                                  |\n",
      "+--------+--------------------------------------+\n",
      "|4898091 |FinancialTimes                        |\n",
      "|7540072 |neufmetres                            |\n",
      "|10575072|Webzine de la dracenie et du Var Est ن|\n",
      "|16683666|spectator                             |\n",
      "|17385313|Julien_W                              |\n",
      "|17437184|alphoenix                             |\n",
      "|17464719|PascalR                               |\n",
      "|17779850|Jennifer Ogor                         |\n",
      "|18976358|SylvainePascual                       |\n",
      "|19713578|chris dabin                           |\n",
      "+--------+--------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_vertices.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faisons maintenant le dataset des arcs :\n",
    "* on prend l'identifiant de l'émetteur\n",
    "* on prend duplique/explose la ligne pour avoir une ligne par mention dans le tweet\n",
    "* on récupère les hashtags\n",
    "* on récupère l'identifiant du tweet\n",
    "\n",
    "on group by user_id et mention.id (user mentionné) et on agggrège pour avoir :\n",
    "* le nombre de tweets\n",
    "* la liste des hashtags vus entre ces 2 users dans le sens user_id -> mention.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count,collect_list,flatten\n",
    "edges=df.select(col(\"user.id\").alias(\"user_id\"), \\\n",
    "               explode(col(\"entities.user_mentions\")).alias(\"mention\"),\n",
    "               col(\"entities.hashtags.text\").alias(\"hashtags\"),\n",
    "               \"id\") \\\n",
    " .groupby(col(\"user_id\").alias(\"src\"), \\\n",
    "          col(\"mention.id\").alias(\"dst\")) \\\n",
    " .agg(count(\"id\").alias(\"nb\"),\n",
    "      flatten(collect_list(\"hashtags\")).alias(\"hashtags\"),\n",
    "      collect_list(\"id\").alias(\"id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+---+--------------------+--------------------+\n",
      "|     src|               dst| nb|            hashtags|                  id|\n",
      "+--------+------------------+---+--------------------+--------------------+\n",
      "|15217683|793749212118867969|  1|                  []|[1380995523168120...|\n",
      "|15872615|         121468512|  1|                  []|[1380245558141591...|\n",
      "|16600674|         217473382|  6|                  []|[1380505509611069...|\n",
      "|17193568|         217473382|  1|          [immigrés]|[1380513459482337...|\n",
      "|17385313|         217473382|  6|                  []|[1380247279894986...|\n",
      "|17464719|        2515649016|  1|                  []|[1380436148057608...|\n",
      "|18629937|         121468512|  1|                  []|[1380492059690295...|\n",
      "|18969131|          20947741|  1|                  []|[1380468019009355...|\n",
      "|19377400|         112754792|  1|                  []|[1381174506446848...|\n",
      "|20064944|        1460135654|  1|                  []|[1380449540738850...|\n",
      "|20181221|          46375782|  1|                  []|[1381281068037369...|\n",
      "|20181221|        1325327106|  1|                  []|[1381281068037369...|\n",
      "|20773587|          26073581|  2|                  []|[1381742535253573...|\n",
      "|20773587|         305783924|  2|                  []|[1381742535253573...|\n",
      "|21040954|          15618138|  1|                  []|[1380511284807012...|\n",
      "|22662958|          97514372|  1|                  []|[1382931299300704...|\n",
      "|23931979|         103918784|  1|                  []|[1380485755651362...|\n",
      "|27016118|983334079981654016|  1|                  []|[1381295795186585...|\n",
      "|28111905|        1187428428|  1|[JDD, Arras, Gran...|[1381340432462970...|\n",
      "|36017082|793749212118867969|  1|                  []|[1380866702695669...|\n",
      "+--------+------------------+---+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "edges.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Créer le graphe\n",
    "\n",
    "Ca y est on a nos 2 structures:\n",
    "* **identifiant**, name ici seul **identifiant** est nécessaire name vient donner plus d'infos on pourrait ajouter d'autres colonnes\n",
    "* **identifianttwittant(src), identifiantmentionné(dst)**, nombre de fois, liste de hashtags, liste des ids de tweets ici seul les 2 premiers identifiants sont nécessaires le reste donne du contexte qu'on pourrait utiliser sur les arcs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphframes import *\n",
    "g = GraphFrame(final_vertices, edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant qu'on a ce graphe on va le mettre en cache car nous allons le manipuler plusieurs fois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphFrame(v:[id: bigint, name: string], e:[src: bigint, dst: bigint ... 3 more fields])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "le graphe a 8407 noeuds et 11423 arcs\n"
     ]
    }
   ],
   "source": [
    "print(\"le graphe a \"+ str(g.vertices.count()) +\" noeuds et \"+ str(g.edges.count()) + \" arcs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recherche des utilisateurs populaires\n",
    "\n",
    "On peut via l'api appliquer différents algorithmes, il est d'usage dans les graphes de traiter des notions suivantes :\n",
    "* la notion de degré (nombre de connexion d'un noeud)\n",
    "* la notion de triangle (triplet de noeud totalement connecté)\n",
    "* la notion de chemin entre noeud\n",
    "\n",
    "L'algorithme pageRank est un algorithme qui est connu pour etre utilisé dans les moteurs de recherche, il peut etre executé sur un graphe pour juger de la \"popularité\" d'un noeud.\n",
    "\n",
    "On s'attend à ce que le comte de l'insee soit pas mal placé.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagerank = g.pageRank(resetProbability=0.15,tol=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------------------------------------+------------------+\n",
      "|id                 |name                                              |pagerank          |\n",
      "+-------------------+--------------------------------------------------+------------------+\n",
      "|103918784          |David Lisnard                                     |917.3584687834675 |\n",
      "|217473382          |Insee                                             |600.8061601474191 |\n",
      "|983334079981654016 |Docteur Peter EL BAZE                             |149.12888199110637|\n",
      "|945473418          |Docteur Laurent Alexandre #JeSuisDoublementVacciné|147.51973826592192|\n",
      "|3373762791         |GWGoldnadel                                       |138.67006416773918|\n",
      "|793749212118867969 |Raphael Pradeau                                   |120.89617748694437|\n",
      "|1243542086860918785|FrontPopOff                                       |115.42839094804633|\n",
      "|382736141          |Enedis                                            |112.99675094123769|\n",
      "|20947741           |Frédéric Bianchi                                  |70.79398229596002 |\n",
      "|1209500610787196929|ObservatoireID                                    |70.0362125440602  |\n",
      "|882991259249455104 |Romain                                            |64.11423919805802 |\n",
      "|814141682040111105 |JS_2ass                                           |64.11423919805802 |\n",
      "|485565224          |InedFr                                            |60.605597337374284|\n",
      "|82607099           |Barbara Serrano                                   |59.36675428454663 |\n",
      "|55509201           |gforestier                                        |52.692516812210336|\n",
      "|268227446          |datagouvfr                                        |52.585970059663204|\n",
      "|472412809          |f_philippot                                       |52.09475147832378 |\n",
      "|465995281          |Tobelive is back                                  |45.88950490102726 |\n",
      "|1325542737979056135|MB 🇲🇽                                           |45.38748673426592 |\n",
      "|1323679162746441730|Lechevalier xavier                                |42.44459114991851 |\n",
      "+-------------------+--------------------------------------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "pagerank.vertices.sort(desc(\"pagerank\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arf seulement 2ème sur la période de temps David Lisnard le maire de Cannes a bien volé la vedette "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---+--------+---------------------+------+\n",
      "|src      |dst      |nb |hashtags|id                   |weight|\n",
      "+---------+---------+---+--------+---------------------+------+\n",
      "|103918784|103918784|1  |[]      |[1380509496284430336]|1.0   |\n",
      "+---------+---------+---+--------+---------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pagerank.edges.where((col(\"src\")==\"103918784\") ).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "510"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pagerank.edges.where((col(\"dst\")==\"103918784\") ).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "David lisnard n'a tweeté qu'une fois mais il s'est fait mentionné 510 fois autour de ce tweet.\n",
    "\n",
    "https://twitter.com/davidlisnard/status/1380509496284430336"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut regarder les utilisateurs ayant le plus été mentionnés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------+--------------------------------------------------+\n",
      "|id                |inDegree|name                                              |\n",
      "+------------------+--------+--------------------------------------------------+\n",
      "|217473382         |1178    |Insee                                             |\n",
      "|945473418         |582     |Docteur Laurent Alexandre #JeSuisDoublementVacciné|\n",
      "|3373762791        |562     |GWGoldnadel                                       |\n",
      "|103918784         |510     |David Lisnard                                     |\n",
      "|983334079981654016|410     |Docteur Peter EL BAZE                             |\n",
      "|793749212118867969|391     |Raphael Pradeau                                   |\n",
      "|472412809         |198     |f_philippot                                       |\n",
      "|978184280         |192     |Damien Rieu                                       |\n",
      "|217749896         |182     |MLP_officiel                                      |\n",
      "|87212906          |167     |ThierryMARIANI                                    |\n",
      "+------------------+--------+--------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g.inDegrees.join(g.vertices,\"id\").orderBy(desc(\"inDegree\")).show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ou qui ont mentionné le plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+------------------------------------------------+\n",
      "|id                 |outDegree|name                                            |\n",
      "+-------------------+---------+------------------------------------------------+\n",
      "|702991804661112832 |22       |Ari Kouts                                       |\n",
      "|1244716894671732738|21       |Bonsens Claire                                  |\n",
      "|1359816991033458689|20       |Cris Finland                                    |\n",
      "|1580838720         |20       |Augusta Crochet 💙LR🇫🇷🇦🇲                    |\n",
      "|1376154610998640650|19       |Nasty Estelle 🇨🇵❤️                            |\n",
      "|1180210966711156736|17       |29quovadis                                      |\n",
      "|1222491562824949763|16       |Jacques                                         |\n",
      "|1284473275184226304|16       |Antoine Sulo                                    |\n",
      "|1378687476697550852|16       |BouleBille                                      |\n",
      "|2464135300         |16       |Daniel pilotte de la BAROLLIER🌼⚜️❣️🦅☦️🇮🇱🇵🇱|\n",
      "+-------------------+---------+------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g.outDegrees.join(g.vertices,\"id\").orderBy(desc(\"outDegree\")).show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'api propose d'autres algorithmes permettant:\n",
    "* de clusteriser le graphe (connected ou strong connected components)\n",
    "* de rechercher des patterns ou chemin dans le graphe ce qui pourrait etre intéressant pour voir par exemple une chaine de retweet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfin on peut nous même créer nos propres algorithmes pour cela l'api propose une méthode aggregate messages permettant d'envoyer un message de noeud et noeud et définir l'aggregation à retenir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est aussi possible d'utiliser d3.js dans une celle ipython et d'afficher le graphe en javascript todo un exemple ici\n",
    "https://bl.ocks.org/heybignick/3faf257bbbbc7743bb72310d03b86ee8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "    var script = document.createElement('script');\n",
       "    script.type = 'text/javascript';\n",
       "    script.src = '//d3js.org/d3.v6.min.js';\n",
       "    document.head.appendChild(script);\n",
       "    console.log(window.d3)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "    var script = document.createElement('script');\n",
    "    script.type = 'text/javascript';\n",
    "    script.src = '//d3js.org/d3.v6.min.js';\n",
    "    document.head.appendChild(script);\n",
    "    console.log(window.d3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "var svg = d3.select(element)\n",
       "    .append(\"svg\")\n",
       "    .attr(\"width\", 300)\n",
       "    .attr(\"height\", 300);    \n",
       "\n",
       "svg.append(\"circle\")\n",
       "    .style(\"stroke\", \"gray\")\n",
       "    .style(\"fill\", \"cyan\")\n",
       "    .attr(\"r\", 130)\n",
       "    .attr(\"cx\", 150)\n",
       "    .attr(\"cy\", 150)\n",
       "    .transition()\n",
       "        .delay(100)\n",
       "        .duration(10000)  \n",
       "        .attr(\"r\", 10)\n",
       "        .attr(\"cx\", 150)\n",
       "        .style(\"fill\", \"blue\"); \n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Javascript\n",
    "svg_script = '''\n",
    "var svg = d3.select(element)\n",
    "    .append(\"svg\")\n",
    "    .attr(\"width\", 300)\n",
    "    .attr(\"height\", 300);    \n",
    "\n",
    "svg.append(\"circle\")\n",
    "    .style(\"stroke\", \"gray\")\n",
    "    .style(\"fill\", \"cyan\")\n",
    "    .attr(\"r\", 130)\n",
    "    .attr(\"cx\", 150)\n",
    "    .attr(\"cy\", 150)\n",
    "    .transition()\n",
    "        .delay(100)\n",
    "        .duration(10000)  \n",
    "        .attr(\"r\", 10)\n",
    "        .attr(\"cx\", 150)\n",
    "        .style(\"fill\", \"blue\"); \n",
    "'''\n",
    "\n",
    "Javascript(svg_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "  const data = {nodes : [{ id: \"Myriel\", group: 1},{ id: \"Myriel3\", group: 1}], \n",
       "  links : [{source: \"Myriel\", target: \"Myriel3\", value: 1}]}\n",
       "\n",
       "  const links = data.links.map(d => Object.create(d));\n",
       "  const nodes = data.nodes.map(d => Object.create(d));\n",
       "  const width=300;\n",
       "  const height=400;\n",
       "  const simulation = d3.forceSimulation(nodes)\n",
       "      .force(\"link\", d3.forceLink(links).id(d => d.id))\n",
       "      .force(\"charge\", d3.forceManyBody())\n",
       "      .force(\"center\", d3.forceCenter(width / 2, height / 2));\n",
       "\n",
       "  const svg = d3.create(\"svg\")\n",
       "      .attr(\"viewBox\", [0, 0, width, height]);\n",
       "\n",
       "  const link = svg.append(\"g\")\n",
       "      .attr(\"stroke\", \"#999\")\n",
       "      .attr(\"stroke-opacity\", 0.6)\n",
       "    .selectAll(\"line\")\n",
       "    .data(links)\n",
       "    .join(\"line\")\n",
       "      .attr(\"stroke-width\", d => Math.sqrt(d.value));\n",
       "\n",
       "  function color ()  {\n",
       "      const scale = d3.scaleOrdinal(d3.schemeCategory10);\n",
       "      return d => scale(d.group);\n",
       "  }\n",
       "  \n",
       "  function drag (simulation) {\n",
       "  \n",
       "  function dragstarted(event) {\n",
       "    if (!event.active) simulation.alphaTarget(0.3).restart();\n",
       "    event.subject.fx = event.subject.x;\n",
       "    event.subject.fy = event.subject.y;\n",
       "  }\n",
       "  \n",
       "  function dragged(event) {\n",
       "    event.subject.fx = event.x;\n",
       "    event.subject.fy = event.y;\n",
       "  }\n",
       "  \n",
       "  function dragended(event) {\n",
       "    if (!event.active) simulation.alphaTarget(0);\n",
       "    event.subject.fx = null;\n",
       "    event.subject.fy = null;\n",
       "  }\n",
       "  \n",
       "  return d3.drag()\n",
       "      .on(\"start\", dragstarted)\n",
       "      .on(\"drag\", dragged)\n",
       "      .on(\"end\", dragended);\n",
       "    }\n",
       "  \n",
       "  const node = svg.append(\"g\")\n",
       "      .attr(\"stroke\", \"#fff\")\n",
       "      .attr(\"stroke-width\", 1.5)\n",
       "    .selectAll(\"circle\")\n",
       "    .data(nodes)\n",
       "    .join(\"circle\")\n",
       "      .attr(\"r\", 5)\n",
       "      .attr(\"fill\", color)\n",
       "      .call(drag(simulation));\n",
       "\n",
       "  node.append(\"title\")\n",
       "      .text(d => d.id);\n",
       "\n",
       "  simulation.on(\"tick\", () => {\n",
       "    link\n",
       "        .attr(\"x1\", d => d.source.x)\n",
       "        .attr(\"y1\", d => d.source.y)\n",
       "        .attr(\"x2\", d => d.target.x)\n",
       "        .attr(\"y2\", d => d.target.y);\n",
       "\n",
       "    node\n",
       "        .attr(\"cx\", d => d.x)\n",
       "        .attr(\"cy\", d => d.y);\n",
       "  });\n",
       "\n",
       " \n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Javascript\n",
    "svg_script = '''\n",
    "  const data = {nodes : [{ id: \"Myriel\", group: 1},{ id: \"Myriel3\", group: 1}], \n",
    "  links : [{source: \"Myriel\", target: \"Myriel3\", value: 1}]}\n",
    "\n",
    "  const links = data.links.map(d => Object.create(d));\n",
    "  const nodes = data.nodes.map(d => Object.create(d));\n",
    "  const width=300;\n",
    "  const height=400;\n",
    "  const simulation = d3.forceSimulation(nodes)\n",
    "      .force(\"link\", d3.forceLink(links).id(d => d.id))\n",
    "      .force(\"charge\", d3.forceManyBody())\n",
    "      .force(\"center\", d3.forceCenter(width / 2, height / 2));\n",
    "\n",
    "  const svg = d3.create(\"svg\")\n",
    "      .attr(\"viewBox\", [0, 0, width, height]);\n",
    "\n",
    "  const link = svg.append(\"g\")\n",
    "      .attr(\"stroke\", \"#999\")\n",
    "      .attr(\"stroke-opacity\", 0.6)\n",
    "    .selectAll(\"line\")\n",
    "    .data(links)\n",
    "    .join(\"line\")\n",
    "      .attr(\"stroke-width\", d => Math.sqrt(d.value));\n",
    "\n",
    "  function color ()  {\n",
    "      const scale = d3.scaleOrdinal(d3.schemeCategory10);\n",
    "      return d => scale(d.group);\n",
    "  }\n",
    "  \n",
    "  function drag (simulation) {\n",
    "  \n",
    "  function dragstarted(event) {\n",
    "    if (!event.active) simulation.alphaTarget(0.3).restart();\n",
    "    event.subject.fx = event.subject.x;\n",
    "    event.subject.fy = event.subject.y;\n",
    "  }\n",
    "  \n",
    "  function dragged(event) {\n",
    "    event.subject.fx = event.x;\n",
    "    event.subject.fy = event.y;\n",
    "  }\n",
    "  \n",
    "  function dragended(event) {\n",
    "    if (!event.active) simulation.alphaTarget(0);\n",
    "    event.subject.fx = null;\n",
    "    event.subject.fy = null;\n",
    "  }\n",
    "  \n",
    "  return d3.drag()\n",
    "      .on(\"start\", dragstarted)\n",
    "      .on(\"drag\", dragged)\n",
    "      .on(\"end\", dragended);\n",
    "    }\n",
    "  \n",
    "  const node = svg.append(\"g\")\n",
    "      .attr(\"stroke\", \"#fff\")\n",
    "      .attr(\"stroke-width\", 1.5)\n",
    "    .selectAll(\"circle\")\n",
    "    .data(nodes)\n",
    "    .join(\"circle\")\n",
    "      .attr(\"r\", 5)\n",
    "      .attr(\"fill\", color)\n",
    "      .call(drag(simulation));\n",
    "\n",
    "  node.append(\"title\")\n",
    "      .text(d => d.id);\n",
    "\n",
    "  simulation.on(\"tick\", () => {\n",
    "    link\n",
    "        .attr(\"x1\", d => d.source.x)\n",
    "        .attr(\"y1\", d => d.source.y)\n",
    "        .attr(\"x2\", d => d.target.x)\n",
    "        .attr(\"y2\", d => d.target.y);\n",
    "\n",
    "    node\n",
    "        .attr(\"cx\", d => d.x)\n",
    "        .attr(\"cy\", d => d.y);\n",
    "  });\n",
    "\n",
    " \n",
    "'''\n",
    "\n",
    "Javascript(svg_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
