{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark graph\n",
    "\n",
    "Dans les tutoriels précédents nous avons toujours travaillé avec des dataset ou table.\n",
    "\n",
    "Il existe un écosystème spark pour traiter de gros graphes via l'api **graphx** historiquement que disponible en scala il existe **graphFrames** en python.\n",
    "\n",
    "Essayons de jouer avec cette api.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création du context Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7fdd5c3bb430>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import os\n",
    "\n",
    "conf = SparkConf()\n",
    "\n",
    "#url par défaut d'une api kubernetes accédé depuis l'intérieur du cluster (ici le notebook tourne lui même dans kubernetes)\n",
    "conf.setMaster(\"k8s://https://kubernetes.default.svc:443\")\n",
    "\n",
    "#image des executors spark: pour des raisons de simplicité on réutilise l'image du notebook\n",
    "conf.set(\"spark.kubernetes.container.image\", os.environ['IMAGE_NAME'])\n",
    "\n",
    "# Nom du compte de service pour contacter l'api kubernetes : attention le package du datalab crée lui même cette variable d'enviromment.\n",
    "# Dans un pod du cluster kubernetes il faut lire le fichier /var/run/secrets/kubernetes.io/serviceaccount/token\n",
    "# Néanmoins ce paramètre est inutile car le contexte kubernetes local de ce notebook est préconfiguré\n",
    "# conf.set(\"spark.kubernetes.authenticate.driver.serviceAccountName\", os.environ['KUBERNETES_SERVICE_ACCOUNT']) \n",
    "\n",
    "# Nom du namespace kubernetes\n",
    "conf.set(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE'])\n",
    "\n",
    "# Nombre d'executeur spark, il se lancera autant de pods kubernetes que le nombre indiqué.\n",
    "conf.set(\"spark.executor.instances\", \"10\")\n",
    "\n",
    "# Mémoire alloué à la JVM\n",
    "# Attention par défaut le pod kubernetes aura une limite supérieur qui dépend d'autres paramètres.\n",
    "# On manipulera plus bas pour vérifier la limite de mémoire totale d'un executeur\n",
    "conf.set(\"spark.executor.memory\", \"4g\")\n",
    "\n",
    "conf.set(\"spark.kubernetes.driver.pod.name\", os.environ['KUBERNETES_POD_NAME'])\n",
    "\n",
    "# Paramètres d'enregistrement des logs spark d'application\n",
    "# Attention ce paramètres nécessitent la création d'un dossier spark-history. Spark ne le fait pas lui même pour des raisons obscurs\n",
    "# import s3fs\n",
    "# endpoint = \"https://\"+os.environ['AWS_S3_ENDPOINT']\n",
    "# fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': endpoint})\n",
    "# fs.touch('s3://tm8enk/spark-history/.keep')\n",
    "# sparkconf.set(\"spark.eventLog.enabled\",\"true\")\n",
    "# sparkconf.set(\"spark.eventLog.dir\",\"s3a://tm8enk/spark-history\")\n",
    "#ici pour gérer le dateTimeFormatter dépendant de la verion de java...\n",
    "conf.set(\"spark.sql.legacy.timeParserPolicy\",\"LEGACY\")\n",
    "conf.set(\"spark.default.parallelism\",10)\n",
    "conf.set(\"spark.sql.shuffle.partitions\",10)\n",
    "conf.set(\"spark.jars.packages\",\"graphframes:graphframes:0.8.1-spark3.0-s_2.12\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On note que :\n",
    "* on a pris 10 executeurs et on surcharge spark pour, lors de shuffle ou repartition, que son niveau de parallelisme soit 10 plutot que 200\n",
    "* on importe un jar au démarrage il va aller le chercher sur maven central, ce jar sera appellé par la librairie python graphFrames à travers Py4j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "graphframes#graphframes added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-4b2d4557-b063-420a-a465-474cfa3a9511;1.0\n",
      "\tconfs: [default]\n",
      "\tfound graphframes#graphframes;0.8.1-spark3.0-s_2.12 in spark-packages\n",
      "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
      "downloading https://repos.spark-packages.org/graphframes/graphframes/0.8.1-spark3.0-s_2.12/graphframes-0.8.1-spark3.0-s_2.12.jar ...\n",
      "\t[SUCCESSFUL ] graphframes#graphframes;0.8.1-spark3.0-s_2.12!graphframes.jar (70ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.16/slf4j-api-1.7.16.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.16!slf4j-api.jar (19ms)\n",
      ":: resolution report :: resolve 3496ms :: artifacts dl 93ms\n",
      "\t:: modules in use:\n",
      "\tgraphframes#graphframes;0.8.1-spark3.0-s_2.12 from spark-packages in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   2   |   2   |   0   ||   2   |   2   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-4b2d4557-b063-420a-a465-474cfa3a9511\n",
      "\tconfs: [default]\n",
      "\t2 artifacts copied, 0 already retrieved (281kB/8ms)\n",
      "2022-03-30 07:21:42,143 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2022-03-30 07:21:44,847 WARN spark.ExecutorAllocationManager: Dynamic allocation without a shuffle service is an experimental feature.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"graph\").config(conf = conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting graphframes\n",
      "  Downloading graphframes-0.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from graphframes) (1.20.3)\n",
      "Collecting nose\n",
      "  Downloading nose-1.3.7-py3-none-any.whl (154 kB)\n",
      "     |████████████████████████████████| 154 kB 7.0 MB/s            \n",
      "\u001b[?25hInstalling collected packages: nose, graphframes\n",
      "Successfully installed graphframes-0.6 nose-1.3.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install graphframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constitution d'un graphe\n",
    "\n",
    "Un graphe peut être vu comme 2 datasets:\n",
    "* Un **dataset de noeud** avec 2 colonnes : un identifiant et un attribut.\n",
    "* Un **dataset d'arcs** reliant 2 noeuds : avec un identifiant de noeud source, un destination et un ou des attributs sur cet arc\n",
    " \n",
    "A partir de ces 2 datasets que spark peut traiter comme des rdd à travers le cluster, on peut travailler sur un graphe, voici un exemple simple :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphframes import *\n",
    "\n",
    "vertices = [(1,\"A\"), (2,\"B\"), (3, \"C\")]\n",
    "edges = [(1,2,\"love\"), (2,1,\"hate\"), (2,3,\"follow\")]\n",
    "\n",
    "v = spark.createDataFrame(vertices, [\"id\", \"name\"])\n",
    "e = spark.createDataFrame(edges, [\"src\", \"dst\", \"action\"])\n",
    "\n",
    "premierGraphe = GraphFrame(v, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:===============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+\n",
      "|src|dst|action|\n",
      "+---+---+------+\n",
      "|  1|  2|  love|\n",
      "|  2|  1|  hate|\n",
      "|  2|  3|follow|\n",
      "+---+---+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "premierGraphe.edges.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faisons un graphe sur les données twitter concernant l'insee\n",
    "\n",
    "Attention les données sont alimentées et à la réexecution du notebook les éléments suivants auront peut-être évolués, on peut filtrer sur la date des tweets si nécessaire avant le 16 avril pour retrouver le déroulé ci-dessous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on importe le schema\n",
    "import pickle\n",
    "schema = pickle.load( open( \"../7-spark-streaming/schema.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 07:22:20,319 WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "#on lit les donnnées\n",
    "df = spark.read.format(\"json\")  \\\n",
    "    .schema(schema) \\\n",
    "    .load(\"s3a://projet-spark-lab/diffusion/tweets/input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essayons de créer le graph des tweets mentionnant l'insee avec comme noeud les user de twitter et comme arc, 2 utilisateurs sont reliés si l'émetteur du tweet a metionné un autre utilisateur dans le tweet\n",
    "\n",
    "Par exemple si alice tweet \"@bob tu vas vu le dernier tweet de l'Insee\" le noeud alice sera relié -> à bob.\n",
    "\n",
    "Nous avons de la chance les données de l'api twitter prémache le travail.\n",
    "\n",
    "Regardons pour commencer dans le tweet de l'api twitter les données suivantes:\n",
    "* l'identififiant du tweet\n",
    "* l'identifiant de l'utilisateur qui tweete\n",
    "* son nombre de followers pour savoir s'il est influent\n",
    "* la liste des hashtags présents dans le tweet (déjà donné par twitter)\n",
    "* une indicatrice pour savoir s'il y a des hashtag \n",
    "* le texte du tweet\n",
    "* les user mentionnés dans le tweet sous forme de tableau (identifiant, name, screen_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(id=1380403542725365762, user_id=1229160600737132544, name='FREDERIC 🇫🇷🐷🍴', followers_count=1981, hashtags=['FakeNews'], has_hashtag=1, text='RT @ThierryMARIANI: Quand @MLP_officiel le dénonce, c’est une #FakeNews.\\nQuand l’@InseeFr le confirme, cela devient hélas une évidence que…', user_mentions=[Row(id=87212906, id_str='87212906', indices=[3, 18], name='Thierry MARIANI', screen_name='ThierryMARIANI'), Row(id=217749896, id_str='217749896', indices=[26, 39], name='Marine Le Pen', screen_name='MLP_officiel'), Row(id=217473382, id_str='217473382', indices=[81, 89], name='Insee', screen_name='InseeFr')])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,explode,size,first\n",
    "#.select(\"id\",\"user.id\",\"user.name\",\"user.followers_count\",\"entities.hashtags\",\"text\")\n",
    "df.select(col(\"id\").alias(\"id\"),\n",
    "         col(\"user.id\").alias(\"user_id\"),\n",
    "         col(\"user.name\").alias(\"name\"),\n",
    "         col(\"user.followers_count\"),\n",
    "         col(\"entities.hashtags.text\").alias(\"hashtags\"),\n",
    "         size(\"entities.hashtags\").alias(\"has_hashtag\"),\n",
    "         col(\"text\"),\n",
    "         col(\"entities.user_mentions\")) \\\n",
    " .filter(col(\"has_hashtag\")>0) \\\n",
    " .head(1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour créer le dataset des noeuds:\n",
    "* on récupére les id et noms des émetteurs de tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vertices\n",
    "tweeters=df.select(col(\"user.id\").alias(\"id\"), col(\"user.name\").alias(\"name\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* On récupère les id et noms des user mentionnés (même s'ils n'ont pas forcément twittés)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_mentionned=df.select(explode(col(\"entities.user_mentions\"))).select(col(\"col.id\").alias(\"id\"),col(\"col.screen_name\").alias(\"name\")).distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On fait l'union des 2 dataframe et on va ne retenir qu'un nom (le nom dans la mention n'est pas toujours exact au nom du compte apparemment ca évitera les doublons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices=tweeters.union(users_mentionned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, collect_list\n",
    "# on définit un udf qui prend le premier élément\n",
    "udf_first = udf(lambda x: x[0])\n",
    "# on groupe l'union des deux dataset par id d'utilisateur et on collect \n",
    "#sous la forme d'une liste leurs noms qui peuvent différer entre les mentions et le compte\n",
    "# on applique l'udf pour ne retenir que le nom en tête\n",
    "\n",
    "final_vertices=vertices.groupby(\"id\").agg(udf_first(collect_list(\"name\")).alias(\"name\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------------+\n",
      "|id     |name                    |\n",
      "+-------+------------------------+\n",
      "|612473 |BBCNews                 |\n",
      "|707913 |Fender                  |\n",
      "|1012981|Jean-Yves Stervinou     |\n",
      "|1162091|PierreCol               |\n",
      "|1349111|Croquignol              |\n",
      "|1536651|Rubin Sfadj             |\n",
      "|1745171|Charles                 |\n",
      "|1994321|FRANCE24                |\n",
      "|2357391|Christophe Prieuur      |\n",
      "|4478161|Adam Curtis stan account|\n",
      "+-------+------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "final_vertices.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faisons maintenant le dataset des arcs :\n",
    "* on prend l'identifiant de l'émetteur\n",
    "* on prend duplique/explose la ligne pour avoir une ligne par mention dans le tweet\n",
    "* on récupère les hashtags\n",
    "* on récupère l'identifiant du tweet\n",
    "\n",
    "on group by user_id et mention.id (user mentionné) et on agggrège pour avoir :\n",
    "* le nombre de tweets\n",
    "* la liste des hashtags vus entre ces 2 users dans le sens user_id -> mention.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count,collect_list,flatten\n",
    "edges=df.select(col(\"user.id\").alias(\"user_id\"), \\\n",
    "               explode(col(\"entities.user_mentions\")).alias(\"mention\"),\n",
    "               col(\"entities.hashtags.text\").alias(\"hashtags\"),\n",
    "               \"id\") \\\n",
    " .groupby(col(\"user_id\").alias(\"src\"), \\\n",
    "          col(\"mention.id\").alias(\"dst\")) \\\n",
    " .agg(count(\"id\").alias(\"nb\"),\n",
    "      flatten(collect_list(\"hashtags\")).alias(\"hashtags\"),\n",
    "      collect_list(\"id\").alias(\"id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:=====================================================>(957 + 1) / 958]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+---+--------+--------------------+\n",
      "|    src|                dst| nb|hashtags|                  id|\n",
      "+-------+-------------------+---+--------+--------------------+\n",
      "|1012981| 973130412628246529|  1|      []|[1385318639994290...|\n",
      "|1349111|          217473382|  1|      []|[1426382322706919...|\n",
      "|1536651|           17510568|  1|      []|[1382995514933850...|\n",
      "|1745171|           16649457|  1|      []|[1436253710137667...|\n",
      "|3639301|         4041503175|  1|      []|[1386964092682964...|\n",
      "|4478161|          217473382|  1|      []|[1433405191211130...|\n",
      "|4478161|         3077516146|  1|      []|[1394763627283066...|\n",
      "|5523462|          268227446|  1|      []|[1394285638040567...|\n",
      "|5891532|            5788732|  1|      []|[1394724638886875...|\n",
      "|6274062|          200659061|  1|      []|[1425104376738336...|\n",
      "|6465822|         2832878397|  1|      []|[1386338289511280...|\n",
      "|6465822|1194947441734361089|  1|      []|[1386338289511280...|\n",
      "|6504012|          393484793|  1|      []|[1384791266144231...|\n",
      "|6574682|          632913960|  1|      []|[1436401329270923...|\n",
      "|6663162|           17510568|  1|      []|[1382996047971106...|\n",
      "|6777512|           41093168|  1|      []|[1432613227507200...|\n",
      "|6812352|          304576847|  1|      []|[1430124525291769...|\n",
      "|6999492|          388870788|  1|      []|[1426563709187641...|\n",
      "|7267072|1260248609423056896|  1|      []|[1435512479963045...|\n",
      "|7400132|         1731352014|  1|      []|[1430811031463514...|\n",
      "+-------+-------------------+---+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "edges.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Créer le graphe\n",
    "\n",
    "Ca y est on a nos 2 structures:\n",
    "* **identifiant**, name ici seul **identifiant** est nécessaire name vient donner plus d'infos on pourrait ajouter d'autres colonnes\n",
    "* **identifianttwittant(src), identifiantmentionné(dst)**, nombre de fois, liste de hashtags, liste des ids de tweets ici seul les 2 premiers identifiants sont nécessaires le reste donne du contexte qu'on pourrait utiliser sur les arcs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphframes import *\n",
    "g = GraphFrame(final_vertices, edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant qu'on a ce graphe on va le mettre en cache car nous allons le manipuler plusieurs fois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphFrame(v:[id: bigint, name: string], e:[src: bigint, dst: bigint ... 3 more fields])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "le graphe a 46124 noeuds et 75911 arcs\n"
     ]
    }
   ],
   "source": [
    "print(\"le graphe a \"+ str(g.vertices.count()) +\" noeuds et \"+ str(g.edges.count()) + \" arcs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recherche des utilisateurs populaires\n",
    "\n",
    "On peut via l'api appliquer différents algorithmes, il est d'usage dans les graphes de traiter des notions suivantes :\n",
    "* la notion de degré (nombre de connexion d'un noeud)\n",
    "* la notion de triangle (triplet de noeud totalement connecté)\n",
    "* la notion de chemin entre noeud\n",
    "\n",
    "L'algorithme pageRank est un algorithme qui est connu pour etre utilisé dans les moteurs de recherche, il peut etre executé sur un graphe pour juger de la \"popularité\" d'un noeud.\n",
    "\n",
    "On s'attend à ce que le comte de l'insee soit pas mal placé.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagerank = g.pageRank(resetProbability=0.15,tol=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------------------------------------+------------------+\n",
      "|id                 |name                                              |pagerank          |\n",
      "+-------------------+--------------------------------------------------+------------------+\n",
      "|217473382          |Insee                                             |2006.2580214097795|\n",
      "|304576847          |maximetandonnet                                   |753.6780909654278 |\n",
      "|1312779074088194049|𝕏𝟚𝔻𝔼𝕄🇹🇬🇬🇭                                |423.54102917543133|\n",
      "|103918784          |davidlisnard                                      |372.26630218296174|\n",
      "|546941531          |Guillaume Duval                                   |346.82728313325487|\n",
      "|983334079981654016 |Docteur Peter EL BAZE                             |336.18135734295106|\n",
      "|1214315619031478272|Mediavenir                                        |335.22836144910286|\n",
      "|1499400284         |J_Bardella                                        |317.94904992322387|\n",
      "|2305392175         |superflameur                                      |229.57756545422825|\n",
      "|1281266476528386052|MoorishFacts                                      |207.01759735668492|\n",
      "|204368725          |Ian Brossat                                       |199.1568888367383 |\n",
      "|114222231          |Yves d'Amécourt                                   |194.1236884848601 |\n",
      "|374184532          |BrunoLeMaire                                      |172.59652109774296|\n",
      "|818847510546546690 |Le Discord insoumis                               |158.3744629682767 |\n",
      "|945473418          |Docteur Laurent Alexandre #JeSuisDoublementVacciné|149.20247470313905|\n",
      "|15618138           |coulmont                                          |147.01327165185148|\n",
      "|382736141          |Enedis                                            |141.79970487088306|\n",
      "|338985020          |Agence France-Presse                              |122.67379002355665|\n",
      "|233532563          |David Perrotin                                    |122.1835088661365 |\n",
      "|3373762791         |GWGoldnadel                                       |119.86888415432534|\n",
      "+-------------------+--------------------------------------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "pagerank.vertices.sort(desc(\"pagerank\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardons pourquoi David Lisnard le maire de Cannes arrive dans le top des influenceurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---+--------+---------------------------------------------------------------+------+\n",
      "|src      |dst      |nb |hashtags|id                                                             |weight|\n",
      "+---------+---------+---+--------+---------------------------------------------------------------+------+\n",
      "|103918784|103918784|3  |[]      |[1380509496284430336, 1380509496284430336, 1380509496284430336]|0.5   |\n",
      "|103918784|114222231|1  |[]      |[1430429793301024771]                                          |0.5   |\n",
      "+---------+---------+---+--------+---------------------------------------------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pagerank.edges.where((col(\"src\")==\"103918784\") ).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "545"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pagerank.edges.where((col(\"dst\")==\"103918784\") ).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "David lisnard n'a tweeté qu'une fois mais il s'est fait mentionné 545 fois autour de ce tweet.\n",
    "\n",
    "https://twitter.com/davidlisnard/status/1380509496284430336"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut regarder les utilisateurs ayant le plus été mentionnés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+--------------------------------------------------+\n",
      "|id                 |inDegree|name                                              |\n",
      "+-------------------+--------+--------------------------------------------------+\n",
      "|217473382          |5159    |Insee                                             |\n",
      "|304576847          |2660    |maximetandonnet                                   |\n",
      "|1499400284         |1163    |J_Bardella                                        |\n",
      "|1214315619031478272|1115    |Mediavenir                                        |\n",
      "|1312779074088194049|983     |𝕏𝟚𝔻𝔼𝕄🇹🇬🇬🇭                                |\n",
      "|983334079981654016 |883     |Docteur Peter EL BAZE                             |\n",
      "|2305392175         |692     |superflameur                                      |\n",
      "|945473418          |632     |Docteur Laurent Alexandre #JeSuisDoublementVacciné|\n",
      "|204368725          |620     |Ian Brossat                                       |\n",
      "|978184280          |605     |DamienRieu                                        |\n",
      "+-------------------+--------+--------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g.inDegrees.join(g.vertices,\"id\").orderBy(desc(\"inDegree\")).show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ou qui ont mentionné le plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+--------------------------------------------------+\n",
      "|id                 |outDegree|name                                              |\n",
      "+-------------------+---------+--------------------------------------------------+\n",
      "|1312779074088194049|150      |𝕏𝟚𝔻𝔼𝕄🇹🇬🇬🇭                                |\n",
      "|891710670856753152 |125      |infos etc                                         |\n",
      "|217473382          |109      |Insee                                             |\n",
      "|1107619640409276417|92       |Trinpk🍓✊ 0.1% #ResistSR                          |\n",
      "|1359816991033458689|91       |Cris Finland                                      |\n",
      "|1209500610787196929|68       |Observatoire de l'immigration et de la démographie|\n",
      "|702991804661112832 |56       |Ari Kouts                                         |\n",
      "|1378687476697550852|53       |BouleBille                                        |\n",
      "|1222491562824949763|53       |Jacques                                           |\n",
      "|1419627065809190918|51       |Benedicte                                         |\n",
      "+-------------------+---------+--------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g.outDegrees.join(g.vertices,\"id\").orderBy(desc(\"outDegree\")).show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'api propose d'autres algorithmes permettant:\n",
    "* de clusteriser le graphe (connected ou strong connected components)\n",
    "* de rechercher des patterns ou chemin dans le graphe ce qui pourrait etre intéressant pour voir par exemple une chaine de retweet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfin on peut nous même créer nos propres algorithmes pour cela l'api propose une méthode aggregate messages permettant d'envoyer un message de noeud et noeud et définir l'aggregation à retenir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 08:05:36,267 WARN k8s.ExecutorPodsWatchSnapshotSource: Kubernetes client has been closed.\n"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
